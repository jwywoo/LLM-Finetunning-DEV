{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ENV"
      ],
      "metadata": {
        "id": "VOxmdKZiopdR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BG-LRakn7lk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "RZcfyCqVosMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "GPljWuz-ouMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Prep"
      ],
      "metadata": {
        "id": "ziHHzg2iLEaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "burnfit_dir = os.path.join(\"/content/drive/MyDrive\", \"Burnfit\")\n",
        "input_dir = os.path.join(burnfit_dir, \"input\")\n",
        "output_dir = os.path.join(burnfit_dir, \"output\")\n",
        "\n",
        "inputs = os.listdir(input_dir)\n",
        "outputs = os.listdir(output_dir)\n",
        "\n",
        "input_user_profile_csv = os.path.join(input_dir, \"input_user.csv\")\n",
        "input_user_1rm_csv = os.path.join(input_dir, \"input_user_rm.csv\")\n",
        "output_increase_rate_csv = os.path.join(output_dir, \"output_increase_rate.csv\")\n",
        "output_weekly_increase_rate_plan_csv = os.path.join(output_dir, \"output_weekly_increase_rate_plan.csv\")\n",
        "\n",
        "input_user_profile_df = pd.read_csv(input_user_profile_csv)\n",
        "input_user_1rm_df = pd.read_csv(input_user_1rm_csv)\n",
        "output_increase_rate_df = pd.read_csv(output_increase_rate_csv)\n",
        "output_weekly_increase_rate_plan_df = pd.read_csv(output_weekly_increase_rate_plan_csv)\n",
        "\n",
        "print(input_user_profile_df.columns)\n",
        "print(input_user_1rm_df.columns)\n",
        "print(output_increase_rate_df.columns)\n",
        "print(output_weekly_increase_rate_plan_df.columns)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gigKlY0jLKZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문\n",
        "def question_formmatter(input_row):\n",
        "  input_text = [\n",
        "        \"## 질문\",\n",
        "        \"-5/3/1 프로그램\",\n",
        "        f\"- **운동경험**: {input_row['운동경험']}\",\n",
        "        f\"- **성별**: {input_row['성별']}\",\n",
        "        f\"- **운동목표**: {input_row['운동목표']}\",\n",
        "        \"- **1RM**:\",\n",
        "        f\"  - 벤치프레스: {input_row['벤치프레스']}\",\n",
        "        f\"  - 스쿼트: {input_row['스쿼트']}\",\n",
        "        f\"  - 데드리프트: {input_row['데드리프트']}\",\n",
        "        f\"  - 오버헤드프레스: {input_row['오버헤드프레스']}\"\n",
        "    ]\n",
        "  return \"\\n\".join(input_text)\n",
        "\n",
        "# 답변\n",
        "def answer_formmatter(output_row):\n",
        "  json_output = {\n",
        "        \"program\": \"5/3/1 프로그램\",\n",
        "        \"init_weight_rate\": str(output_row[\"init_weight_rate\"]),\n",
        "        \"increase_rate_week\": str(output_row[\"increase_rate_week\"]),\n",
        "        \"increase_rate_set\": str(output_row[\"increase_rate_set\"]),\n",
        "        \"deloading_rate\": str(output_row[\"deloading_rate\"]),\n",
        "        \"weekly_weight_increase_plan\": output_row[\"weekly_increase_rate_plan\"]\n",
        "  }\n",
        "  answer = f\"## 답변\\n{json.dumps(json_output, ensure_ascii=False, indent=2)}\\n<END>\"\n",
        "  # return f\"## 답변\\n{json.dumps(json_output, ensure_ascii=False, indent=2)}\"\n",
        "  return answer\n",
        "\n",
        "\n",
        "## 질문 + 답변\n",
        "def question_answer_formmatter(input_user_profile_df, input_user_1rm_df, output_increase_rate_df, output_weekly_increase_rate_plan_df):\n",
        "    rows = []\n",
        "    combined_df = input_user_profile_df.merge(input_user_1rm_df, on=\"id\") \\\n",
        "            .merge(output_increase_rate_df, on=\"id\") \\\n",
        "            .merge(output_weekly_increase_rate_plan_df, on=\"id\")\n",
        "\n",
        "    for _, row in combined_df.iterrows():\n",
        "        input_row = row[[\"운동경험\", \"성별\", \"운동목표\", \"벤치프레스\", \"스쿼트\", \"데드리프트\", \"오버헤드프레스\"]]\n",
        "        output_row = row[[\"init_weight_rate\", \"increase_rate_week\", \"increase_rate_set\", \"deloading_rate\", \"weekly_increase_rate_plan\"]]\n",
        "\n",
        "        question = question_formmatter(input_row)\n",
        "        answer = answer_formmatter(output_row)\n",
        "        full_text = f\"{question}\\n\\n{answer}\"\n",
        "        rows.append({\"id\": row[\"id\"], \"text\": full_text})\n",
        "        if _ < 5:\n",
        "          print(full_text)\n",
        "    final_df = pd.DataFrame(rows)\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "zfXlYjuJMpwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_df = question_answer_formmatter(input_user_profile_df, input_user_1rm_df, output_increase_rate_df, output_weekly_increase_rate_plan_df)\n",
        "print(\"Rows:\", text_df.shape[0])\n",
        "print(\"Columns:\", text_df.shape[1])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wfx8pwPtP7jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Process"
      ],
      "metadata": {
        "id": "XaSAC7evLB4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"EleutherAI/polyglot-ko-1.3b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "p5_nIS39dcgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "jBGV-SVAeaPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "print(\"Rows:\", text_df.shape[0])\n",
        "print(\"Columns:\", text_df.shape[1])\n",
        "\n",
        "text_df[[\"text\"]].to_json(\"training_data.jsonl\", orient=\"records\", lines=True, force_ascii=False)"
      ],
      "metadata": {
        "id": "wE4VajpYece4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"training_data.jsonl\")[\"train\"]\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)"
      ],
      "metadata": {
        "id": "K9dcQnkdeenO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path\n",
        "fine_tunned_dir = os.path.join(burnfit_dir, \"fine_tunned\")\n",
        "path_to_save = os.path.join(fine_tunned_dir, \"lora-output-v2\")"
      ],
      "metadata": {
        "id": "kQkONxeIfH53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "## END token\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<END>\"]})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=path_to_save,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,\n",
        "    logging_steps=20,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    optim=\"paged_adamw_8bit\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "VRXOQBKyfFk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training!!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "NghxLWvhf3-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "nRjEK66UpJWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before fine-tuning"
      ],
      "metadata": {
        "id": "-36psZTnhK2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "model_name = \"EleutherAI/polyglot-ko-1.3b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "dlxM2MFqztG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"## 질문\n",
        "- 5/3/1프로그램\n",
        "- **운동경험**: 1\n",
        "- **성별**: 남성\n",
        "- **운동목표**: 6\n",
        "- **1RM**:\n",
        "  - 벤치프레스: 30kg\n",
        "  - 스쿼트: 30kg\n",
        "  - 데드리프트: 30kg\n",
        "  - 오버헤드프레스: 30kg\n",
        "\n",
        "## 답변\n",
        "\"\"\"\n",
        "\n",
        "output = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1,\n",
        ")\n",
        "\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "WK-w-9Rx0crh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## after fine-tuning"
      ],
      "metadata": {
        "id": "aSEM1NI4hOFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### V1"
      ],
      "metadata": {
        "id": "UW_Hhw5W7cyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from peft import PeftModel\n",
        "\n",
        "fine_tunned_dir = os.path.join(burnfit_dir, \"fine_tunned\")\n",
        "path_to_v1 = os.path.join(fine_tunned_dir, \"lora-output-v1\")\n",
        "checkpoint = os.path.join(path_to_v1, \"checkpoint-186\")\n",
        "print(checkpoint)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/polyglot-ko-1.3b\", device_map=\"auto\")\n",
        "lora_model = PeftModel.from_pretrained(base_model, checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=lora_model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "uPyXuPdEhSXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"## 질문\n",
        "- 5/3/1프로그램\n",
        "- **운동경험**: 3\n",
        "- **성별**: 남성\n",
        "- **운동목표**: 1\n",
        "- **1RM**:\n",
        "  - 벤치프레스: 60kg\n",
        "  - 스쿼트: 70kg\n",
        "  - 데드리프트: 70kg\n",
        "  - 오버헤드프레스: 70kg\n",
        "\n",
        "## 답변\n",
        "\"\"\"\n",
        "\n",
        "output = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=260,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1,\n",
        ")\n",
        "\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "88R90neKiJoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### V2: Adding Special Token"
      ],
      "metadata": {
        "id": "7svgBRaB7ib9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import os\n",
        "import torch\n",
        "\n",
        "path_to_v2 = os.path.join(fine_tunned_dir, \"lora-output-v2\")\n",
        "checkpoint = os.path.join(path_to_v2, \"checkpoint-186\")\n",
        "print(checkpoint)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<END>\"]})\n",
        "tokenizer.eos_token = \"<END>\"\n",
        "eos_token_id = tokenizer.convert_tokens_to_ids(\"<END>\")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/polyglot-ko-1.3b\", device_map=\"auto\")\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, checkpoint)"
      ],
      "metadata": {
        "id": "E0a2d1Y67h8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"## 질문\n",
        "- 5/3/1프로그램\n",
        "- **운동경험**: 3\n",
        "- **성별**: 남성\n",
        "- **운동목표**: 2\n",
        "- **1RM**:\n",
        "  - 벤치프레스: 60kg\n",
        "  - 스쿼트: 70kg\n",
        "  - 데드리프트: 70kg\n",
        "  - 오버헤드프레스: 70kg\n",
        "\n",
        "## 답변\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "inputs.pop(\"token_type_ids\", None)\n",
        "\n",
        "outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=300,\n",
        "        eos_token_id=eos_token_id,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.7,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "\n",
        "# generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "# print(generated_text)\n",
        "decoded = tokenizer.decode(outputs[0])\n",
        "\n",
        "print(decoded.split(\"<END>\")[0] + \"<END>\")"
      ],
      "metadata": {
        "id": "85Irf2fZApKG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}